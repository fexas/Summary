# ðŸŽ² The g-and-k Distribution: The "Unsolvable" Puzzle

Welcome to the **g-and-k Experiment**!

If the Gaussian experiment was a warm-up, this is the main event. We are stepping into the world of **intractable likelihoods**â€”problems where traditional statistical methods hit a brick wall.

The **g-and-k distribution** is a notorious benchmark in the simulation-based inference (SBI) community. It's easy to simulate but impossible to write down its probability density function (PDF) in a closed form.

---

## ðŸŽ­ The Cast (The Setup)

We are playing the same game of **Inverse Inference**, but the generator is much more complex.

### 1. The Secret Parameters ($\theta$)
We have 4 knobs that control the shape of the distribution:
1.  **$A$ (Location):** Where the center is.
2.  **$B$ (Scale):** How spread out it is (must be positive).
3.  **$g$ (Skewness):** How much it leans to the left or right.
4.  **$k$ (Kurtosis):** How heavy the tails are (how likely extreme events are).

**The Priors (Our Initial Guess):**
We test three levels of prior knowledge to see how much the method relies on having good initial guesses.

**1. Vague Prior (The "I Have No Clue" Setting)**
*   *Interpretation:* We barely know anything about the parameters. They could be huge, negative, or tiny.
*   **$A$ (Location):** $[-10.0, 10.0]$
*   **$B$ (Scale):** $[0.1, 10.0]$
*   **$g$ (Skewness):** $[-5.0, 5.0]$
*   **$k$ (Kurtosis):** $[-0.4, 5.0]$

**2. Weakly Informative Prior (The "Educated Guess" Setting - Default)**
*   *Interpretation:* We know the parameters represent reasonable physical quantities (e.g., scale isn't 1000, kurtosis isn't crazy high).
*   **$A$ (Location):** $[0.0, 6.0]$ (True Value: 3.0)
*   **$B$ (Scale):** $[0.5, 3.0]$ (True Value: 1.0)
*   **$g$ (Skewness):** $[0.0, 4.0]$ (True Value: 2.0)
*   **$k$ (Kurtosis):** $[0.0, 2.0]$ (True Value: 0.5)

**3. Informative Prior (The "Cheat Mode" Setting)**
*   *Interpretation:* We are already very sure about the values. The search space is tiny.
*   **$A$ (Location):** $[2.5, 3.5]$
*   **$B$ (Scale):** $[0.8, 1.2]$
*   **$g$ (Skewness):** $[1.5, 2.5]$
*   **$k$ (Kurtosis):** $[0.3, 0.7]$

### 2. The Generator (The Simulator)
This is the heart of the problem. It transforms a simple bell curve (standard normal) into a complex, twisted shape.

**The Recipe:**
1.  Take a standard normal random number: $z \sim \mathcal{N}(0, 1)$.
2.  Apply the **g-and-k transformation**:

$$
x = A + B \left( 1 + c \cdot \tanh\left(\frac{g \cdot z}{2}\right) \right) \cdot z \cdot (1 + z^2)^k
$$

*   **$c = 0.8$**: A fixed constant convention.
*   The $\tanh$ term creates **skewness** ($g$).
*   The $(1+z^2)^k$ term creates **heavy tails** ($k$).

### 3. The Clues (The Data)
We observe a set of **100 samples** generated by this process.
*   **Observation:** $X = \{x_1, x_2, ..., x_{100}\}$
*   Each $x_i$ is just a single number (scalar), but together they form a distribution shape.

---

## ðŸ§© The Challenge: Why is this hard?

In the Gaussian case, we had a formula for probability $p(x|\theta)$. We could just calculate it.

Here, **there is no formula for $p(x|\theta)$**.
Because the transformation is so complex, we cannot work backwards to find the probability density. This means we **cannot** use standard methods like MCMC (Markov Chain Monte Carlo) directly.

We **must** use simulation. We have to learn from trial and error: "If I set $A=3, g=2$, does the data look like my observation?"

---

## ðŸ› ï¸ The Solution: SMMD to the Rescue

We use SMMD to learn the inverse mapping directly.

### 1. The Summary Network (DeepSets)
We feed the set of 100 numbers into a specialized neural network called **DeepSets**.
*   **Input:** Set of 100 scalars.
*   **Architecture:** It treats the set as a "cloud" (order doesn't matter). It uses **Invariant** and **Equivariant** layers to process the points.
*   **Output:** It compresses the 100 numbers into **10 Summary Statistics** ($p=10$).

### 2. The Generator Network
*   **Input:** The 10 summary stats + Random Noise ($z$).
*   **Job:** Guess the original 4 parameters ($A, B, g, k$).
*   **Architecture:** A standard MLP with 3 layers of 128 neurons.

### 3. Training Details
*   **Reference Table:** We pre-generate **10,000** simulations to train on.
*   **Batch Size:** 256.
*   **Epochs:** 100.
*   **Loss Function:** Sliced MMD with **50** approximation samples ($M=50$) and **20** slicing directions ($L=20$).

---

## ðŸ† The Goal

If successful, our model will look at a new set of 100 numbers and instantly tell us:
> *"This data is skewed ($g=2.0$) and has heavy tails ($k=0.5$). It definitely didn't come from a normal distribution."*

This proves SMMD can handle **complex, non-standard distributions** where traditional math fails.
