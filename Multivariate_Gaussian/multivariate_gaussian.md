# ðŸŒŒ The Multivariate Gaussian Experiment: A Quest for Variance

Welcome to the **Multivariate Gaussian Experiment**! 

Imagine you are a detective. You walk into a room and see a cloud of points floating in the air. Your job? Figure out how "puffed out" this cloud is. This "puffiness" is what we call the **variance** ($\sigma^2$). 

This experiment tests how well our method, **SMMD (Sliced Maximum Mean Discrepancy)**, can learn to estimate this hidden variance, even when the data lives in very high-dimensional spaces (like 100 dimensions!).

---

## ðŸŽ¯ The Mission (The Setup)

We are playing a game of **Inverse Inference**. 

### 1. The Secret Parameter ($\theta$)
This is the number we want to find. In our case, it's the **variance** ($\sigma^2$) of a Gaussian distribution.
- **The Rules:** The variance isn't just any number. We assume it comes from a specific range.
- **The Prior:** $\sigma^2 \sim U(0.1, 9.0)$
  > *Plain English:* "We know the variance is somewhere between 0.1 and 9.0, but all values in that range are equally likely."

### 2. The Generator (The Simulator)
This is the machine that creates the data. If you feed it a variance ($\sigma^2$), it spits out data points.
- **The Formula:** $x \sim \mathcal{N}(0, \sigma^2 I_d)$
  > *Plain English:* "The data points are centered at zero. They are scattered in all directions ($d$ dimensions) equally, and the spread is determined by our secret variance $\sigma^2$."

### 3. The Clues (The Data)
We don't see $\sigma^2$ directly. We only see a bag of **1000 data points** generated by it.
- **The Observation:** $X = \{x_1, x_2, ..., x_{1000}\}$ where each $x_i$ is a vector with $d$ numbers.

---

## ðŸ”ï¸ The Challenge: The Curse of Dimensionality

Here is the twist: We make the game harder by increasing the number of dimensions ($d$).

1.  **Level 1 (Easy):** $d=2$. The points are on a 2D sheet of paper.
2.  **Level 2 (Medium):** $d=10$. The points are in a 10-dimensional hyper-space.
3.  **Level 3 (Hard):** $d=100$. The points are in a massive 100-dimensional space!

Standard methods often struggle as dimensions go up because the space becomes incredibly vast and empty. We want to see if **SMMD** can stay cool under pressure.

---

## ðŸ› ï¸ The Solution: SMMD in Action

Our SMMD method doesn't try to look at all 1000 points in 100 dimensions at once. That would be overwhelming. Instead, it learns to **summarize** the data.

### Step 1: The Summary Network
It takes the bag of 1000 high-dimensional points and compresses them into just **2 numbers** (Summary Statistics).
- **Why 2?** Ideally, we only need 1 number (the empirical variance) to estimate the true variance. Giving it 2 slots gives the network a little breathing room to find the best way to compress the information.

### Step 2: The Generator Network
It takes those **2 summary numbers** (plus some random noise) and tries to guess the original $\sigma^2$.

### Step 3: The Sliced MMD Loss
This is the "teacher" that corrects the network.
- It compares the distribution of the *guessed* variances with the *true* variances.
- **"Sliced":** It projects the complex high-dimensional differences onto simple 1D lines, making it much easier and faster to calculate.

---

## ðŸ“Š The Math (Translated)

Here is the formal definition of what's happening under the hood:

**The Likelihood (The probability of seeing the data given the variance):**
$$
p(x|\sigma^2) = \prod_{i=1}^{N} \frac{1}{\sqrt{(2\pi\sigma^2)^d}} \exp\left(-\frac{\|x_i\|^2}{2\sigma^2}\right)
$$

> *Translation:* "The probability of seeing these specific points ($x$) drops as they get further from the center (that's the $\exp$ part). The $\sigma^2$ controls how quickly that probability drops."

**The Posterior (What we want to find):**
$$
p(\sigma^2|X) \propto p(X|\sigma^2) \cdot p(\sigma^2)
$$

> *Translation:* "Given the data points $X$ we see, what is the most likely value for $\sigma^2$?"

---

## ðŸš€ Why This Experiment Matters

If SMMD works, it should be able to look at a cloud of points in 100 dimensions and say:
> *"Ah, based on the spread of these points, I am 95% sure the variance is 4.0."*

And it should do this just as well in 100 dimensions as it does in 2 dimensions. This proves that SMMD is **robust**, **efficient**, and ready for real-world problems where data is complex and high-dimensional.
