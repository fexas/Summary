%% 2019 %%

@inproceedings{Kolouri2019GSW,
       author = {{Kolouri}, S. and {Nadjahi}, K. and {Simsekli}, U. and
         {Badeau}, R. and {Rohde}, G. K.},
        title = "{Generalized Sliced Wasserstein Distances}",
        booktitle = {Advances in Neural Information Processing Systems},
        year = "2019"
}


@article{Bernton2019,
author = {Bernton, E. and Jacob, P. E. and Gerber, M. and Robert, C. P.},
title = {Approximate Bayesian computation with the Wasserstein distance},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {81},
number = {2},
pages = {235-269},
keywords = {Approximate Bayesian computation, Generative models, Likelihood-free inference, Optimal transport, Wasserstein distance},
doi = {10.1111/rssb.12312},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12312},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12312},
abstract = {Summary A growing number of generative statistical models do not permit the numerical evaluation of their likelihood functions. Approximate Bayesian computation has become a popular approach to overcome this issue, in which one simulates synthetic data sets given parameters and compares summaries of these data sets with the corresponding observed values. We propose to avoid the use of summaries and the ensuing loss of information by instead using the Wasserstein distance between the empirical distributions of the observed and synthetic data. This generalizes the well-known approach of using order statistics within approximate Bayesian computation to arbitrary dimensions. We describe how recently developed approximations of the Wasserstein distance allow the method to scale to realistic data sizes, and we propose a new distance based on the Hilbert space filling curve. We provide a theoretical study of the method proposed, describing consistency as the threshold goes to 0 while the observations are kept fixed, and concentration properties as the number of observations grows. Various extensions to time series data are discussed. The approach is illustrated on various examples, including univariate and multivariate g-and-k distributions, a toggle switch model from systems biology, a queuing model and a Lévy-driven stochastic volatility model.},
year = {2019}
}


@article{peyre2019computational,
  title={Computational optimal transport},
  author={Peyr{\'e}, G. and Cuturi, M. and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{fan2019brief,
  title={Brief review of image denoising techniques},
  author={Fan, L. and Zhang, F. and Fan, H. and Zhang, C.},
  journal={Visual Computing for Industry, Biomedicine, and Art},
  volume={2},
  number={1},
  pages={7},
  year={2019},
  publisher={Springer}
}

@inproceedings{martin2001database,
  title={A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics},
  author={Martin, D. and Fowlkes, C. and Tal, D. and Malik, J. and others},
  year={2001},
  organization={Iccv Vancouver:}
}

@misc{swabc_code,
    author = {Nadjahi, K. and De Bortoli, V. and Durmus, A. and Badeau, R. and \c{S}im\c{s}ekli, U. },
    title = {SW-ABC software implementation},
    howpublished = {\url{https://github.com/kimiandj/slicedwass_abc}, \url{https://vdeborto.github.io/publication/sw_abc}}
    }

@inproceedings{2019arXiv190604516N,
       author = {{Nadjahi}, K. and {Durmus}, A. and
         {{\c{S}}im{\textcommabelow s}ekli}, U. and {Badeau}, R.},
        title = "{Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance}",
        booktitle = {Advances in Neural Information Processing Systems},
        year = "2019"
}

@inproceedings{deshpande2019max,
  title={Max-{S}liced {W}asserstein Distance and its use for {GAN}s},
  author={Deshpande, I. and Hu, Y.-T. and Sun, R. and Pyrros, A. and Siddiqui, N. and Koyejo, S. and Zhao, Z. and Forsyth, D. and Schwing, A.},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2019}
}

@inproceedings{Kolouri2019Sliced,
title={Sliced {W}asserstein Auto-Encoders},
author={S. Kolouri and P. E. Pope and C. E. Martin and G. K. Rohde},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1xaJn05FQ},
}


@article{Weed2019,
author = "Weed, J. and Bach, F.",
doi = "10.3150/18-BEJ1065",
fjournal = "Bernoulli",
journal = "Bernoulli",
month = "11",
number = "4A",
pages = "2620--2648",
publisher = "Bernoulli Society for Mathematical Statistics and Probability",
title = "Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance",
url = "https://doi.org/10.3150/18-BEJ1065",
volume = "25",
year = "2019"
}


%% 2018 %%

@ARTICLE{Sisson2018,
       author = {{Sisson}, S.~A. and {Fan}, Y. and {Beaumont}, M.~A.},
        title = "{Overview of Approximate Bayesian Computation}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Computation, Statistics - Methodology, Statistics - Machine Learning},
         year = "2018",
        month = "Feb",
          eid = {arXiv:1802.09720},
        pages = {arXiv:1802.09720},
archivePrefix = {arXiv},
       eprint = {1802.09720},
 primaryClass = {stat.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180209720S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Frazier2018,
    author = {Frazier, D. T. and Martin, G. M. and Robert, C.P. and Rousseau, J.},
    title = "{Asymptotic properties of approximate Bayesian computation}",
    journal = {Biometrika},
    volume = {105},
    number = {3},
    pages = {593-607},
    year = {2018},
    month = {06},
    abstract = "{Approximate Bayesian computation allows for statistical analysis using models with intractable likelihoods. In this paper we consider the asymptotic behaviour of the posterior distribution obtained by this method. We give general results on the rate at which the posterior distribution concentrates on sets containing the true parameter, the limiting shape of the posterior distribution, and the asymptotic distribution of the posterior mean. These results hold under given rates for the tolerance used within the method, mild regularity conditions on the summary statistics, and a condition linked to identification of the true parameters. Implications for practitioners are discussed.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asy027},
    url = {https://doi.org/10.1093/biomet/asy027},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/105/3/593/25470059/asy027.pdf},
}

@article{Klinger2018,
    author = {Klinger, E. and Rickert, D. and Hasenauer, J.},
    title = "{pyABC: distributed, likelihood-free inference}",
    journal = {Bioinformatics},
    volume = {34},
    number = {20},
    pages = {3591-3593},
    year = {2018},
    month = {05},
    abstract = "{Likelihood-free methods are often required for inference in systems biology. While approximate Bayesian computation (ABC) provides a theoretical solution, its practical application has often been challenging due to its high computational demands. To scale likelihood-free inference to computationally demanding stochastic models, we developed pyABC: a distributed and scalable ABC-Sequential Monte Carlo (ABC-SMC) framework. It implements a scalable, runtime-minimizing parallelization strategy for multi-core and distributed environments scaling to thousands of cores. The framework is accessible to non-expert users and also enables advanced users to experiment with and to custom implement many options of ABC-SMC schemes, such as acceptance threshold schedules, transition kernels and distance functions without alteration of pyABC’s source code. pyABC includes a web interface to visualize ongoing and finished ABC-SMC runs and exposes an API for data querying and post-processing.pyABC is written in Python 3 and is released under a 3-clause BSD license. The source code is hosted on https://github.com/icb-dcm/pyabc and the documentation on http://pyabc.readthedocs.io. It can be installed from the Python Package Index (PyPI).Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bty361},
    url = {https://doi.org/10.1093/bioinformatics/bty361},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/34/20/3591/26024383/bty361.pdf},
}

@InProceedings{Jiang18,
  title =    {Approximate Bayesian Computation with Kullback-Leibler Divergence as Data Discrepancy},
  author =   {Jiang, B. and Wu, T.-Y. and Wong, W.-H.},
  booktitle =    {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages =    {1711--1721},
  year =   {2018},
  editor =   {Amos Storkey and Fernando Perez-Cruz},
  volume =   {84},
  series =   {Proceedings of Machine Learning Research},
  address =    {Playa Blanca, Lanzarote, Canary Islands},
  month =    {09--11 Apr},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v84/jiang18a/jiang18a.pdf},
  url =    {http://proceedings.mlr.press/v84/jiang18a.html},
  abstract =   {Complex simulator-based models usually have intractable likelihood functions, rendering the likelihood-based inference methods inapplicable. Approximate Bayesian Computation (ABC) emerges as an alternative framework of likelihood-free inference methods. It identifies a quasi-posterior distribution by finding values of parameter that simulate the synthetic data resembling the observed data. A major ingredient of ABC is the discrepancy measure between the observed and the simulated data, which conventionally involves a fundamental difficulty of constructing effective summary statistics. To bypass this difficulty, we adopt a Kullback-Leibler divergence estimator to assess the data discrepancy. Our method enjoys the asymptotic consistency and linearithmic time complexity as the data size increases. In experiments on five benchmark models, this method achieves a comparable or higher quasi-posterior quality, compared to the existing methods using other discrepancy measures.}
}

@inproceedings{Deshpande2018,
  title={Generative modeling using the sliced {W}asserstein distance},
  author={Deshpande, I. and Zhang, Z. and Schwing, A. G.},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3483--3491},
  year={2018}
}

@inproceedings{csimcsekli2018sliced,
  title={Sliced-{W}asserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions},
  author={Liutkus, A. and {\c{S}}im{\c{s}}ekli, U. and Majewski, S. and Durmus, A. and Stoter, F.-R.},
  booktitle={International Conference on Machine Learning},
  year={2019}
}


%% 2016 %%

@InProceedings{Park16,
  title =    {K2-ABC: Approximate Bayesian Computation with Kernel Embeddings},
  author =   {M. Park and W. Jitkrittum and D. Sejdinovic},
  booktitle =    {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages =    {398--407},
  year =   {2016},
  editor =   {Arthur Gretton and Christian C. Robert},
  volume =   {51},
  series =   {Proceedings of Machine Learning Research},
  address =    {Cadiz, Spain},
  month =    {09--11 May},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v51/park16.pdf},
  url =    {http://proceedings.mlr.press/v51/park16.html},
  abstract =   {Complicated generative models often result in a situation where computing the likelihood of observed data is intractable, while simulating from the conditional density given a parameter value is relatively easy. Approximate Bayesian Computation (ABC) is a paradigm that enables simulation-based posterior inference in such cases by measuring the similarity between simulated and observed data in terms of a chosen set of summary statistics. However, there is no general rule to construct sufficient summary statistics for complex models. Insufficient summary statistics will leak information, which leads to ABC algorithms yielding samples from an incorrect  posterior. In this paper, we propose a fully nonparametric ABC paradigm which circumvents the need for manually selecting summary statistics. Our approach, K2-ABC,  uses maximum mean discrepancy (MMD) to construct a dissimilarity measure between the observed and simulated data. The embedding of an empirical distribution of the data into a reproducing kernel Hilbert space plays a role of the summary statistic and is sufficient whenever the corresponding kernels are characteristic. Experiments on a simulated scenario and a real-world biological problem illustrate the effectiveness of the proposed algorithm.}
}


%% 2015 %%

@article{bonneel2015sliced,
  title={Sliced and {R}adon {W}asserstein barycenters of measures},
  author={Bonneel, N. and Rabin, J. and Peyr{\'e}, G. and Pfister, H.},
  journal={Journal of Mathematical Imaging and Vision},
  volume={51},
  number={1},
  pages={22--45},
  year={2015},
  publisher={Springer}
}


%% 2013 %%

@phdthesis{Bonnotte2013,
    title    = {Unidimensional and Evolution Methods for Optimal Transportation},
    school   = {Paris 11},
    author   = {N. Bonnotte},
    year     = {2013}
}


%% 2012 %%

@Article{DelMoral2012,
author="Del Moral, P.
and Doucet, A.
and Jasra, A.",
title="An adaptive sequential Monte Carlo method for approximate Bayesian computation",
journal="Statistics and Computing",
year="2012",
month="Sep",
day="01",
volume="22",
number="5",
pages="1009--1020",
abstract="Approximate Bayesian computation (ABC) is a popular approach to address inference problems where the likelihood function is intractable, or expensive to calculate. To improve over Markov chain Monte Carlo (MCMC) implementations of ABC, the use of sequential Monte Carlo (SMC) methods has recently been suggested. Most effective SMC algorithms that are currently available for ABC have a computational complexity that is quadratic in the number of Monte Carlo samples (Beaumont et al., Biometrika 86:983--990, 2009; Peters et al., Technical report, 2008; Toni et al., J. Roy. Soc. Interface 6:187--202, 2009) and require the careful choice of simulation parameters. In this article an adaptive SMC algorithm is proposed which admits a computational complexity that is linear in the number of samples and adaptively determines the simulation parameters. We demonstrate our algorithm on a toy example and on a birth-death-mutation model arising in epidemiology.",
issn="1573-1375",
doi="10.1007/s11222-011-9271-y",
url="https://doi.org/10.1007/s11222-011-9271-y"
}


@article{Fearnhead2012,
author = {Fearnhead, P. and Prangle, D.},
title = {Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {74},
number = {3},
pages = {419-474},
keywords = {Indirect inference, Likelihood-free inference, Markov chain Monte Carlo methods, Simulation, Stochastic kinetic networks},
doi = {10.1111/j.1467-9868.2011.01010.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2011.01010.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2011.01010.x},
abstract = {Summary.  Many modern statistical applications involve inference for complex stochastic models, where it is easy to simulate from the models, but impossible to calculate likelihoods. Approximate Bayesian computation (ABC) is a method of inference for such models. It replaces calculation of the likelihood by a step which involves simulating artificial data for different parameter values, and comparing summary statistics of the simulated data with summary statistics of the observed data. Here we show how to construct appropriate summary statistics for ABC in a semi-automatic manner. We aim for summary statistics which will enable inference about certain parameters of interest to be as accurate as possible. Theoretical results show that optimal summary statistics are the posterior means of the parameters. Although these cannot be calculated analytically, we use an extra stage of simulation to estimate how the posterior means vary as a function of the data; and we then use these estimates of our summary statistics within ABC. Empirical results show that our approach is a robust method for choosing summary statistics that can result in substantially more accurate ABC analyses than the ad hoc choices of summary statistics that have been proposed in the literature. We also demonstrate advantages over two alternative methods of simulation-based inference.},
year = {2012}
}



%% 2011 %%

@InProceedings{rabin:et:al:2011,
author="Rabin, J.
and Peyr{\'e}, G.
and Delon, J.
and Bernot, M.",
title="Wasserstein Barycenter and Its Application to Texture Mixing",
booktitle="Scale Space and Variational Methods in Computer Vision",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="435--446",
abstract="This paper proposes a new definition of the averaging of discrete probability distributions as a barycenter over the Monge-Kantorovich optimal transport space. To overcome the time complexity involved by the numerical solving of such problem, the original Wasserstein metric is replaced by a sliced approximation over 1D distributions. This enables us to introduce a new fast gradient descent algorithm to compute Wasserstein barycenters of point clouds.",
isbn="978-3-642-24785-9"
}


%% 2010 %%
@article{Wood2010,
title = "Statistical inference for noisy nonlinear ecological dynamic systems",
abstract = "Chaotic ecological dynamic systems defy conventional statistical analysis. Systems with near-chaotic dynamics are little better. Such systems are almost invariably driven by endogenous dynamic processes plus demographic and environmental process noise, and are only observable with error. Their sensitivity to history means that minute changes in the driving noise realization, or the system parameters, will cause drastic changes in the system trajectory(1). This sensitivity is inherited and amplified by the joint probability density of the observable data and the process noise, rendering it useless as the basis for obtaining measures of statistical fit. Because the joint density is the basis for the fit measures used by all conventional statistical methods(2), this is a major theoretical shortcoming. The inability to make well-founded statistical inferences about biological dynamic models in the chaotic and near-chaotic regimes, other than on an ad hoc basis, leaves dynamic theory without the methods of quantitative validation that are essential tools in the rest of biological science. Here I show that this impasse can be resolved in a simple and general manner, using a method that requires only the ability to simulate the observed data on a system from the dynamic model about which inferences are required. The raw data series are reduced to phase-insensitive summary statistics, quantifying local dynamic structure and the distribution of observations. Simulation is used to obtain the mean and the covariance matrix of the statistics, given model parameters, allowing the construction of a 'synthetic likelihood' that assesses model fit. This likelihood can be explored using a straightforward Markov chain Monte Carlo sampler, but one further post-processing step returns pure likelihood-based inference. I apply the method to establish the dynamic nature of the fluctuations in Nicholson's classic blowfly experiments(3-5).",
author = "Wood, {S. N.}",
year = "2010",
month = "8",
day = "26",
doi = "10.1038/nature09319",
language = "English",
volume = "466",
pages = "1102--1104",
journal = "Nature",
issn = "0028-0836",
publisher = "Nature Publishing Group",
number = "7310",
}


%% 2009 %% 
@article{Toni2009,
author = {Toni, T. and Welch, D. and Strelkowa, N. and Ipsen, A. and Stumpf, M.},
year = {2009},
month = {03},
pages = {187-202},
title = {Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems},
volume = {6},
journal = {Journal of the Royal Society, Interface / the Royal Society},
doi = {10.1098/rsif.2008.0172}
}


%% 2008 %%

@book{Villani2008,
    author = {Villani, C.},
    citeulike-article-id = {5975037},
    day = {30},
    edition = {2009},
    howpublished = {Hardcover},
    isbn = {3540710493},
    month = sep,
    posted-at = {2010-06-08 15:50:09},
    priority = {5},
    publisher = {Springer},
    series = {Grundlehren der mathematischen Wissenschaften},
    title = {{Optimal Transport: Old and New}},
    url = {http://www.worldcat.org/isbn/3540710493},
    year = {2008}
}


%% 2006 %%
@article{Peters2006,
author = {Peters, G. and Sisson, S.},
year = {2006},
month = {09},
pages = {},
title = {Bayesian inference, Monte Carlo sampling and operational risk},
volume = {1},
journal = {Journal of Operational Risk},
doi = {10.21314/JOP.2006.014}
}

@article{Tanaka2006,
author = {Tanaka, M. and Francis, A. and Luciani, F. and Sisson, S.},
year = {2006},
month = {08},
pages = {1511-20},
title = {Using Approximate Bayesian Computation to Estimate Tuberculosis Transmission Parameters From Genotype Data},
volume = {173},
journal = {Genetics},
doi = {10.1534/genetics.106.055574}
}


%% 2002 %%

@article {Beaumont2002,
  author = {Beaumont, M. A. and Zhang, W. and Balding, D. J.},
  title = {Approximate Bayesian Computation in Population Genetics},
  volume = {162},
  number = {4},
  pages = {2025--2035},
  year = {2002},
  publisher = {Genetics},
  abstract = {We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.},
  issn = {0016-6731},
  URL = {https://www.genetics.org/content/162/4/2025},
  eprint = {https://www.genetics.org/content/162/4/2025.full.pdf},
  journal = {Genetics}
}

%% 1999 %%

@book{Billingsley1999,
  added-at = {2009-04-24T23:33:01.000+0200},
  address = {New York},
  author = {Billingsley, P.},
  biburl = {https://www.bibsonomy.org/bibtex/2657f92e619abe605188197d74b27f572/peter.ralph},
  description = {q-paper},
  edition = {Second},
  interhash = {555ad867bdd2f4e0824bffe13fa1b9f9},
  intrahash = {657f92e619abe605188197d74b27f572},
  isbn = {0-471-19745-9},
  keywords = {probability_theory reference},
  mrclass = {60B10 (28A33 60F17)},
  mrnumber = {MR1700749 (2000e:60008)},
  note = {A Wiley-Interscience Publication},
  pages = {x+277},
  publisher = {John Wiley \& Sons Inc.},
  series = {Wiley Series in Probability and Statistics: Probability and
              Statistics},
  timestamp = {2009-04-24T23:44:01.000+0200},
  title = {Convergence of probability measures},
  year = 1999
}


%% 1998 %%

@book {rachev:ruschendorf:1998,
    AUTHOR = {Rachev, S. T. and R\"{u}schendorf, L.},
     TITLE = {Mass transportation problems. {V}ol. {I}},
    SERIES = {Probability and its Applications (New York)},
      NOTE = {Theory},
 PUBLISHER = {Springer-Verlag, New York},
      YEAR = {1998},
     PAGES = {xxvi+508},
      ISBN = {0-387-98350-3},
   MRCLASS = {28A35 (28C15 49K27 60B05 90B15)},
  MRNUMBER = {1619170},
MRREVIEWER = {Vladimir L. Levin},
}

%% 1997 %%
@article {Tavare97,
  author = {Tavar{\'e}, S. and Balding, D. J. and Griffiths, R. C. and Donnelly, P.},
  title = {Inferring Coalescence Times From DNA Sequence Data},
  volume = {145},
  number = {2},
  pages = {505--518},
  year = {1997},
  publisher = {Genetics},
  abstract = {The paper is concerned with methods for the estimation of the coalescence time (time since the most recent common ancestor) of a sample of intraspecies DNA sequences. The methods take advantage of prior knowledge of population demography, in addition to the molecular data. While some theoretical results are presented, a central focus is on computational methods. These methods are easy to implement, and, since explicit formulae tend to be either unavailable or unilluminating, they are also more useful and more informative in most applications. Extensions are presented that allow for the effects of uncertainty in our knowledge of population size and mutation rates, for variability in population sizes, for regions of different mutation rate, and for inference concerning the coalescence time of the entire population. The methods are illustrated using recent data from the human Y chromosome.},
  issn = {0016-6731},
  URL = {https://www.genetics.org/content/145/2/505},
  eprint = {https://www.genetics.org/content/145/2/505.full.pdf},
  journal = {Genetics}
}


%% 1958 %%

@article{Varadarajan58,
 ISSN = {00364452},
 URL = {http://www.jstor.org/stable/25048365},
 author = {V. S. Varadarajan},
 journal = {Sankhyā: The Indian Journal of Statistics (1933-1960)},
 number = {1/2},
 pages = {23--26},
 publisher = {Springer},
 title = {On the Convergence of Sample Probability Distributions},
 volume = {19},
 year = {1958}
}


@article{launay2018exact,
  title={Exact Sampling of Determinantal Point Processes without Eigendecomposition},
  author={Launay, C. and Galerne, B. and Desolneux, A.},
  journal={arXiv preprint arXiv:1802.08429},
  year={2018}
}

@inproceedings{buades2005non,
  title={A non-local algorithm for image denoising},
  author={Buades, A. and Coll, B. and Morel, J.-M.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={2},
  pages={60--65},
  year={2005},
  organization={IEEE}
}

@inproceedings{kervrann2014approximate,
  title={Approximate Bayesian Computation, stochastic algorithms and non-local means for complex noise models},
  author={Kervrann, C. and Roudot, P. and Waharte, F.},
  booktitle={2014 IEEE International Conference on Image Processing (ICIP)},
  pages={2834--2838},
  year={2014},
  organization={IEEE}
}

@article{de2019patch,
  title={Patch Redundancy in Images: A Statistical Testing Framework and Some Applications},
  author={De Bortoli, V. and Desolneux, A. and Galerne, B. and Leclaire, A.},
  journal={SIAM Journal on Imaging Sciences},
  volume={12},
  number={2},
  pages={893--926},
  year={2019},
  publisher={SIAM}
}